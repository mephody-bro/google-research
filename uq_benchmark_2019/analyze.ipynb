{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from array_utils import load_npz, write_npz\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "softmax = scipy.special.softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# path = 'mnist/results/stats_0.npz'\n",
    "# load_npz(path)\n",
    "\n",
    "data = {\"lol\": np.array([10])}\n",
    "\n",
    "write_npz('mnist/results', 'test.npz', data)\n",
    "load_npz('mnist/results/test.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from metrics_lib import compute_accuracies_at_confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 50, endpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "stats = load_npz('mnist/results/svi/stats_small_28.npz')\n",
    "accuracies, count = compute_accuracies_at_confidences(stats['labels'], stats['probs'], thresholds)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(thresholds, count)\n",
    "plt.subplot(122)\n",
    "plt.plot(thresholds, accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ensemble_stats(ensemble_dir, stat_name, ensemble_size):\n",
    "    probs = None\n",
    "    for i in range(ensemble_size):\n",
    "        stats = load_npz(os.path.join(ensemble_dir, str(i), stat_name))\n",
    "        if probs is None:\n",
    "            probs = stats['probs']\n",
    "        else:\n",
    "            probs += stats['probs']\n",
    "        print(sum(probs[0]))\n",
    "    probs /= ensemble_size\n",
    "    return {'probs': probs, 'labels': stats['labels']}\n",
    "    \n",
    "stats = generate_ensemble_stats('mnist/results/ensemble', 'stats_small_0.npz', 15)\n",
    "write_npz('mnist/results/ensemble/', 'stats_small_28.npz', stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement BALD approach for dropout confidence\n",
    "def bald_confidence(stats):   \n",
    "    probs = stats['probs']\n",
    "    probs_entropy = - np.sum(probs * np.log(probs + 1e-10), axis=-1)\n",
    "    \n",
    "    samples = softmax(stats['logits_samples'], axis=-1)\n",
    "    samples_entropy = - np.mean(np.sum(samples * np.log(samples + 1e-10), axis=-1), axis=1)\n",
    "    \n",
    "    confidence = -probs_entropy + samples_entropy + 1\n",
    "    confidence[confidence < 0] = 0\n",
    "    \n",
    "    print(confidence[:10])\n",
    "    print(np.max(confidence))\n",
    "    print(np.min(confidence))\n",
    "    return confidence\n",
    "    \n",
    "    \n",
    "#     print(generated[0], stats['probs'][0])\n",
    "    \n",
    "                                   \n",
    "    \n",
    "cf = bald_confidence(load_npz('mnist/results/dropout/stats_28.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidences(confidences, thresholds):\n",
    "    shape = (len(thresholds),)\n",
    "    counts = np.zeros(shape)\n",
    "\n",
    "#   eq = np.equal(predict_class, labels)\n",
    "    for i, thresh in enumerate(thresholds):\n",
    "        mask = confidences >= thresh\n",
    "        counts[i] = mask.sum(-1)\n",
    "    return counts\n",
    "\n",
    "compute_confidences(cf, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "ood_name = 'stats_small_28.npz'\n",
    "methods = ['vanilla', 'svi', 'dropout', 'll_svi', 'll_dropout', 'ensemble']\n",
    "bald_methods = ['dropout', 'll_dropout']\n",
    "\n",
    "counts = {}\n",
    "\n",
    "for method in methods:\n",
    "    path = os.path.join('mnist/results/',method, ood_name)\n",
    "    stats = load_npz(path)\n",
    "    _, counts[method] = compute_accuracies_at_confidences(stats['labels'], stats['probs'], thresholds)\n",
    "    \n",
    "for method in bald_methods:\n",
    "    path = os.path.join('mnist/results/',method, 'stats_28.npz')\n",
    "    stats = load_npz(path)\n",
    "    confidence = compute_confidences(bald_confidence(stats), thresholds)\n",
    "    print('shape conf', confidence.shape)\n",
    "    counts[method+'_bald'] = confidence\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for method, count in counts.items():\n",
    "    print(method, count)\n",
    "    plt.plot(thresholds, count, label=method)\n",
    "plt.title('Confidence for OOD')\n",
    "plt.xlabel(r'$\\tau$')\n",
    "plt.ylabel(r'Number of example p(y|x) > $\\tau$')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "method = 'dropout'\n",
    "path = os.path.join('mnist/results/',method, 'stats_28.npz')\n",
    "stats = load_npz(path)\n",
    "# _, counts[method] = compute_accuracies_at_confidences(stats['labels'], stats['probs'], thresholds)\n",
    "stats['logits_samples'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
